# Настройка дефолтных параметров LLM-прокси

Этот план описывает, как добавить поддержку дефолтных параметров генерации
в сервисе `backend/service/main.py` и покрыть изменения тестами.

## Цели
- Дать администраторам возможность задавать дефолтную температуру и лимит
  токенов через переменные окружения.
- Гарантировать корректную валидацию значений и документировать новые
  настройки.
- Дополнить тесты, чтобы зафиксировать ожидаемое поведение.

## Шаги реализации
1. **Расширить конфигурацию.**
   - В `Settings` добавить поля `llm_temperature_default` и
     `llm_max_tokens_default`.
   - В `Settings.load()` читать переменные окружения
     `KOLIBRI_LLM_TEMPERATURE` и `KOLIBRI_LLM_MAX_TOKENS`, валидировать
     значения и сохранять их в новых полях.
2. **Использовать дефолты при запросе.**
   - В `_perform_upstream_call` перед формированием `payload` подставлять
     значения из настроек, если в запросе нет явных параметров.
3. **Обновить документацию.**
   - Расширить таблицу переменных окружения в `README.md`, добавив
     описание новых ключей и допустимые диапазоны значений.
4. **Покрыть тестами.**
   - Добавить кейсы в `tests/test_backend_service.py`, проверяющие что
     значения по умолчанию прокидываются к апстрим-сервису.
   - Добавить тесты на защиту от некорректных значений переменных
     окружения.
5. **Проверить изменения.**
   - Выполнить `pytest tests/test_backend_service.py -q`.

Следуя этим шагам, мы улучшим UX администраторов и сократим риск
регрессий.
