"""FastAPI application that powers the Kolibri enterprise backend."""
from __future__ import annotations

import json
import time
from typing import Any, Dict, Optional

import httpx
from fastapi import Depends, FastAPI, Form, HTTPException, Response, status
from pydantic import BaseModel, Field
from .actions import (
    ActionCatalogResponse,
    ActionMacro,
    ActionMacroListResponse,
    ActionMacroPayload,
    ActionRunRequest,
    ActionRunResult,
    get_macro_store,
    get_orchestrator,
)
from .metrics import (
    CONTENT_TYPE_LATEST,
    CollectorRegistry,
    generate_latest,
    register_counter,
    register_histogram,
)

from .audit import log_audit_event, log_genome_event
from .config import Settings, get_settings
from .security import AuthContext, issue_session_token, parse_saml_response, require_permission

app = FastAPI(title="Kolibri Enterprise API", version="1.0.0")

registry = CollectorRegistry()
INFER_REQUESTS = register_counter(
    registry,
    "kolibri_infer_requests_total",
    "Количество запросов к апстрим LLM",
    labelnames=("outcome",),
)
INFER_LATENCY = register_histogram(
    registry,
    "kolibri_infer_latency_seconds",
    "Латентность запроса к LLM",
    labelnames=("provider",),
    buckets=(0.05, 0.1, 0.25, 0.5, 1.0, 2.0, 5.0, 10.0),
)
SSO_EVENTS = register_counter(
    registry,
    "kolibri_sso_events_total",
    "Количество событий SSO",
    labelnames=("event",),
)


class HealthResponse(BaseModel):
    status: str = "ok"
    response_mode: str
    sso_enabled: bool
    prometheus_namespace: str


class InferenceRequest(BaseModel):
    prompt: str = Field(min_length=1, description="User prompt that should be answered by the model")
    mode: Optional[str] = Field(default=None, description="Frontend-selected response mode")
    temperature: Optional[float] = Field(default=None, ge=0.0, le=2.0)
    max_tokens: Optional[int] = Field(default=None, ge=1)


class InferenceResponse(BaseModel):
    response: str = Field(description="Text generated by the LLM")
    provider: str = Field(description="Identifier of the upstream provider")
    latency_ms: Optional[float] = Field(default=None, description="End-to-end latency for the upstream call")


class SAMLLoginResponse(BaseModel):
    token: str
    subject: str
    expires_at: Optional[float]
    relay_state: Optional[str]


def _extract_text(payload: Any) -> str:
    if isinstance(payload, dict):
        if isinstance(payload.get("response"), str):
            return payload["response"].strip()

        choices = payload.get("choices")
        if isinstance(choices, list) and choices:
            first_choice = choices[0]
            if isinstance(first_choice, dict):
                message = first_choice.get("message")
                if isinstance(message, dict) and isinstance(message.get("content"), str):
                    return message["content"].strip()
                if isinstance(first_choice.get("text"), str):
                    return first_choice["text"].strip()

        if isinstance(payload.get("content"), str):
            return payload["content"].strip()

    raise ValueError("Upstream response did not contain text output")


async def _perform_upstream_call(
    request: InferenceRequest,
    settings: Settings,
) -> tuple[str, float, str]:
    if not settings.llm_endpoint:
        raise HTTPException(
            status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
            detail="LLM endpoint is not configured",
        )

    headers: Dict[str, str] = {"Content-Type": "application/json"}
    if settings.llm_api_key:
        headers["Authorization"] = f"Bearer {settings.llm_api_key}"

    temperature = request.temperature
    if temperature is None:
        temperature = settings.llm_temperature_default

    max_tokens = request.max_tokens
    if max_tokens is None:
        max_tokens = settings.llm_max_tokens_default

    payload: Dict[str, Any] = {
        "prompt": request.prompt,
        "mode": request.mode,
        "model": settings.llm_model,
        "temperature": temperature,
        "max_tokens": max_tokens,
    }

    sanitized_payload = {key: value for key, value in payload.items() if value is not None}

    start = time.perf_counter()
    async with httpx.AsyncClient(timeout=settings.llm_timeout) as client:
        upstream_response = await client.post(
            settings.llm_endpoint,
            json=sanitized_payload,
            headers=headers,
        )
    elapsed = (time.perf_counter() - start) * 1000.0

    try:
        upstream_response.raise_for_status()
    except httpx.HTTPStatusError as exc:
        detail = exc.response.text
        raise HTTPException(
            status_code=status.HTTP_502_BAD_GATEWAY,
            detail=f"Upstream LLM returned {exc.response.status_code}: {detail}",
        ) from exc

    try:
        payload_json = upstream_response.json()
    except json.JSONDecodeError as exc:
        raise HTTPException(
            status_code=status.HTTP_502_BAD_GATEWAY,
            detail="Upstream LLM responded with invalid JSON",
        ) from exc

    try:
        text = _extract_text(payload_json)
    except ValueError as exc:
        raise HTTPException(
            status_code=status.HTTP_502_BAD_GATEWAY,
            detail=str(exc),
        ) from exc

    provider = "llm"
    if isinstance(payload_json, dict) and isinstance(payload_json.get("provider"), str):
        provider = payload_json["provider"].strip() or provider

    return text, elapsed, provider


@app.get("/api/health", response_model=HealthResponse)
async def health(settings: Settings = Depends(get_settings)) -> HealthResponse:
    return HealthResponse(
        status="ok",
        response_mode=settings.response_mode,
        sso_enabled=settings.sso_enabled,
        prometheus_namespace=settings.prometheus_namespace,
    )


@app.get("/metrics")
async def metrics() -> Response:
    payload = generate_latest(registry)
    return Response(content=payload, media_type=CONTENT_TYPE_LATEST)


@app.get("/api/v1/sso/saml/metadata")
async def saml_metadata(settings: Settings = Depends(get_settings)) -> Response:
    acs_url = settings.saml_acs_url or "/api/v1/sso/saml/acs"
    metadata = f"""
<EntityDescriptor xmlns=\"urn:oasis:names:tc:SAML:2.0:metadata\" entityID=\"{settings.saml_entity_id}\">
  <SPSSODescriptor WantAssertionsSigned=\"true\" AuthnRequestsSigned=\"false\" protocolSupportEnumeration=\"urn:oasis:names:tc:SAML:2.0:protocol\">
    <NameIDFormat>urn:oasis:names:tc:SAML:1.1:nameid-format:emailAddress</NameIDFormat>
    <AssertionConsumerService Binding=\"urn:oasis:names:tc:SAML:2.0:bindings:HTTP-POST\" Location=\"{acs_url}\" index=\"1\" isDefault=\"true\"/>
  </SPSSODescriptor>
</EntityDescriptor>
""".strip()
    return Response(content=metadata, media_type="application/samlmetadata+xml")


@app.post("/api/v1/sso/saml/acs", response_model=SAMLLoginResponse)
async def saml_acs(
    saml_response: str = Form(..., alias="SAMLResponse"),
    relay_state: Optional[str] = Form(default=None, alias="RelayState"),
    settings: Settings = Depends(get_settings),
) -> SAMLLoginResponse:
    context = parse_saml_response(saml_response, settings)
    try:
        token = issue_session_token(context, settings)
    except RuntimeError as exc:  # pragma: no cover - configuration error path
        raise HTTPException(status.HTTP_500_INTERNAL_SERVER_ERROR, detail=str(exc)) from exc

    SSO_EVENTS.labels(event="login").inc()
    log_audit_event(
        event_type="sso.login",
        actor=context.subject,
        payload={"roles": sorted(context.roles), "relay_state": relay_state},
        settings=settings,
    )
    log_genome_event(
        stage="sso",
        actor=context.subject,
        payload={"session_id": context.session_id, "expires_at": context.session_expires_at},
        settings=settings,
    )
    return SAMLLoginResponse(
        token=token,
        subject=context.subject,
        expires_at=context.session_expires_at,
        relay_state=relay_state,
    )


@app.get("/api/v1/sso/session", response_model=SAMLLoginResponse)
async def saml_session(context: AuthContext = Depends(require_permission("kolibri.audit.read"))) -> SAMLLoginResponse:
    return SAMLLoginResponse(
        token="redacted",
        subject=context.subject,
        expires_at=context.session_expires_at,
        relay_state=None,
    )


@app.post("/api/v1/infer", response_model=InferenceResponse)
async def infer(
    request: InferenceRequest,
    settings: Settings = Depends(get_settings),
    context: AuthContext = Depends(require_permission("kolibri.infer")),
) -> InferenceResponse:
    if settings.response_mode != "llm":
        raise HTTPException(
            status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
            detail="LLM mode is disabled",
        )

    try:
        text, latency_ms, provider = await _perform_upstream_call(request, settings)
    except HTTPException:
        INFER_REQUESTS.labels(outcome="error").inc()
        log_audit_event(
            event_type="llm.infer.error",
            actor=context.subject,
            payload={"mode": request.mode},
            settings=settings,
        )
        raise

    INFER_REQUESTS.labels(outcome="success").inc()
    INFER_LATENCY.labels(provider=provider or "unknown").observe((latency_ms or 0.0) / 1000.0)

    log_audit_event(
        event_type="llm.infer",
        actor=context.subject,
        payload={
            "mode": request.mode,
            "provider": provider,
            "latency_ms": latency_ms,
        },
        settings=settings,
    )
    log_genome_event(
        stage="response",
        actor=context.subject,
        payload={"provider": provider, "latency_ms": latency_ms},
        settings=settings,
    )
    return InferenceResponse(response=text, provider=provider, latency_ms=latency_ms)


@app.get("/api/v1/actions/catalog", response_model=ActionCatalogResponse)
async def actions_catalog(
    context: AuthContext = Depends(require_permission("kolibri.actions.run")),
) -> ActionCatalogResponse:
    orchestrator = get_orchestrator()
    recipes = orchestrator.list_descriptors()
    categories = sorted({category for recipe in recipes for category in recipe.categories})
    tags = sorted({tag for recipe in recipes for tag in recipe.tags})
    return ActionCatalogResponse(recipes=recipes, categories=categories, tags=tags)


@app.post("/api/v1/actions/run", response_model=ActionRunResult)
async def run_action(
    request: ActionRunRequest,
    context: AuthContext = Depends(require_permission("kolibri.actions.run")),
) -> ActionRunResult:
    orchestrator = get_orchestrator()
    return await orchestrator.execute(request.action, request.parameters, subject=context.subject)


@app.get("/api/v1/actions/macros", response_model=ActionMacroListResponse)
async def list_macros(
    context: AuthContext = Depends(require_permission("kolibri.actions.run")),
) -> ActionMacroListResponse:
    store = get_macro_store()
    items = await store.list(context.subject)
    return ActionMacroListResponse(items=items)


@app.post("/api/v1/actions/macros", response_model=ActionMacro)
async def create_macro(
    payload: ActionMacroPayload,
    context: AuthContext = Depends(require_permission("kolibri.actions.macros")),
) -> ActionMacro:
    store = get_macro_store()
    return await store.upsert(context.subject, payload)


@app.put("/api/v1/actions/macros/{macro_id}", response_model=ActionMacro)
async def update_macro(
    macro_id: str,
    payload: ActionMacroPayload,
    context: AuthContext = Depends(require_permission("kolibri.actions.macros")),
) -> ActionMacro:
    store = get_macro_store()
    return await store.upsert(context.subject, payload, macro_id=macro_id)


@app.delete("/api/v1/actions/macros/{macro_id}", status_code=status.HTTP_204_NO_CONTENT)
async def delete_macro(
    macro_id: str,
    context: AuthContext = Depends(require_permission("kolibri.actions.macros")),
) -> Response:
    store = get_macro_store()
    await store.delete(context.subject, macro_id)
    return Response(status_code=status.HTTP_204_NO_CONTENT)


app.add_event_handler("shutdown", get_settings.cache_clear)
