"""Pydantic schemas shared between Kolibri service routes."""
from __future__ import annotations

from typing import List, Optional

from pydantic import BaseModel, Field

__all__ = [
    "HealthResponse",
    "InferenceRequest",
    "ModerationDiagnostics",
    "InferenceResponse",
    "SAMLLoginResponse",
]


class HealthResponse(BaseModel):
    status: str = "ok"
    response_mode: str
    sso_enabled: bool
    prometheus_namespace: str

class ModerationDiagnostics(BaseModel):
    tone: str = Field(description="Estimated tone label produced by the moderation pipeline")
    tone_score: float = Field(description="Score in the range [0, 1] representing negativity intensity")
    paraphrased: bool = Field(default=False, description="Whether the response was paraphrased to reduce toxicity")
    negative_terms: List[str] = Field(default_factory=list, description="Negative lexicon matches detected in the text")
    positive_terms: List[str] = Field(default_factory=list, description="Positive lexicon matches detected in the text")


class InferenceRequest(BaseModel):
    prompt: str = Field(min_length=1, description="User prompt that should be answered by the model")
    mode: Optional[str] = Field(default=None, description="Frontend-selected response mode")
    temperature: Optional[float] = Field(default=None, ge=0.0, le=2.0)
    max_tokens: Optional[int] = Field(default=None, ge=1)


class InferenceResponse(BaseModel):
    response: str = Field(description="Text generated by the LLM")
    provider: str = Field(description="Identifier of the upstream provider")
    latency_ms: Optional[float] = Field(default=None, description="End-to-end latency for the upstream call")
    moderation: Optional[ModerationDiagnostics] = Field(
        default=None,
        description="Diagnostics provided by the moderation pipeline",
    )


class SAMLLoginResponse(BaseModel):
    token: str
    subject: str
    expires_at: Optional[float]
    relay_state: Optional[str]
