"""Pydantic schemas shared between Kolibri service routes."""
from __future__ import annotations

from typing import Any, Dict, List, Optional

from pydantic import BaseModel, Field

__all__ = [
    "HealthResponse",
    "InferenceRequest",
    "ModerationDiagnostics",
    "InferenceResponse",
    "IntentCandidate",
    "IntentClassificationRequest",
    "IntentClassificationResponse",
    "PromptTemplateView",
    "SAMLLoginResponse",
]


class HealthResponse(BaseModel):
    status: str = "ok"
    response_mode: str
    sso_enabled: bool
    prometheus_namespace: str

class ModerationDiagnostics(BaseModel):
    tone: str = Field(description="Estimated tone label produced by the moderation pipeline")
    tone_score: float = Field(description="Score in the range [0, 1] representing negativity intensity")
    paraphrased: bool = Field(default=False, description="Whether the response was paraphrased to reduce toxicity")
    negative_terms: List[str] = Field(default_factory=list, description="Negative lexicon matches detected in the text")
    positive_terms: List[str] = Field(default_factory=list, description="Positive lexicon matches detected in the text")


class InferenceRequest(BaseModel):
    prompt: str = Field(min_length=1, description="User prompt that should be answered by the model")
    mode: Optional[str] = Field(default=None, description="Frontend-selected response mode")
    temperature: Optional[float] = Field(default=None, ge=0.0, le=2.0)
    max_tokens: Optional[int] = Field(default=None, ge=1)


class InferenceResponse(BaseModel):
    response: str = Field(description="Text generated by the LLM")
    provider: str = Field(description="Identifier of the upstream provider")
    latency_ms: Optional[float] = Field(default=None, description="End-to-end latency for the upstream call")
    moderation: Optional[ModerationDiagnostics] = Field(
        default=None,
        description="Diagnostics provided by the moderation pipeline",
    )


class IntentCandidate(BaseModel):
    intent: str = Field(description="Predicted intent identifier")
    confidence: float = Field(ge=0.0, le=1.0)


class PromptTemplateView(BaseModel):
    id: str = Field(description="Unique prompt identifier")
    intent: str = Field(description="Intent the prompt is designed for")
    title: str = Field(description="Prompt title ready for display")
    body: str = Field(description="Prompt text that can be inserted into the composer")
    tags: List[str] = Field(default_factory=list, description="Semantic tags for the prompt")


class IntentClassificationRequest(BaseModel):
    text: str = Field(min_length=1, description="User utterance that needs intent classification")
    context: List[str] = Field(default_factory=list, description="Additional context messages")
    top_k: int = Field(default=3, ge=1, le=5, description="Number of intent candidates to return")
    variant: Optional[str] = Field(
        default=None,
        pattern=r"^[abAB]$",
        description="Experiment variant override",
    )
    language: Optional[str] = Field(
        default=None,
        pattern=r"^[a-zA-Z-]{2,5}$",
        description="Preferred language for prompts",
    )


class IntentClassificationResponse(BaseModel):
    intent: str = Field(description="Primary intent identifier")
    confidence: float = Field(ge=0.0, le=1.0)
    variant: str = Field(description="Experiment variant that supplied prompts")
    candidates: List[IntentCandidate] = Field(default_factory=list)
    prompts: List[PromptTemplateView] = Field(default_factory=list)
    settings: Dict[str, Any] = Field(default_factory=dict)


class SAMLLoginResponse(BaseModel):
    token: str
    subject: str
    expires_at: Optional[float]
    relay_state: Optional[str]
